{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6e5fa7",
   "metadata": {},
   "source": [
    "Final File for USA models\n",
    "- complete feature set / data set\n",
    "- 5 models (XGBoost / SVM / Logistic Regression / LSTM / Random Forest)\n",
    "- Every model is ran without Transaction Cost (Other file with TC)\n",
    "- A rolling window is implemented, in contrary to a growing window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341789a2",
   "metadata": {},
   "source": [
    "<h1>Data Loading & Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9d7905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\4033720674.py:29: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_surprise = pd.read_csv('US_economic_releases_events.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date: 1997-09-10 00:01:00, End Date: 2024-12-19 16:00:00\n",
      "Script is ready. Please load your 'df_combined' DataFrame and call 'analyze_and_add_volatility(df_combined)' to start.\n",
      "\n",
      "'df_combined' detected. Running analysis...\n",
      "Calculating daily realized volatility using a '5min' sampling interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jop Brouwer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 'daily realized volatility' column to the DataFrame...\n",
      "\n",
      "'daily realized volatility' column has been added successfully.\n"
     ]
    }
   ],
   "source": [
    "# load the ES_part_x files up to number 11 into df_es\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_es = pd.DataFrame()\n",
    "\n",
    "for i in range(1, 12):\n",
    "    file_name = f'ES_part_{i}.csv'\n",
    "    try:\n",
    "        df_part = pd.read_csv(file_name)\n",
    "        df_es = pd.concat([df_es, df_part], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found. Skipping.\")\n",
    "\n",
    "\n",
    "# set Date column to datetime\n",
    "df_es['Date'] = pd.to_datetime(df_es['Date'], errors='coerce')\n",
    "\n",
    "# set Date as index and combine date with Time column so that it is a datetime index\n",
    "df_es['DateTime'] = pd.to_datetime(df_es['Date'].astype(str) + ' ' + df_es['Time'], errors='coerce')\n",
    "df_es.set_index('DateTime', inplace=True)\n",
    "# drop the original Date and Time columns\n",
    "df_es.drop(columns=['Date', 'Time'], inplace=True)\n",
    "# sort the index\n",
    "df_es.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# Surprise Dataframe\n",
    "# Load the Surprise dataset\n",
    "df_surprise = pd.read_csv('US_economic_releases_events.csv')\n",
    "\n",
    "df_surprise.drop(columns=['S', 'Month', 'Surv(A)', 'Surv(H)', 'Surv(L)', 'Flag', 'Country/Region', 'Day', 'C', 'Category','Subcategory', 'Period', 'Actual'], inplace=True)\n",
    "# Drop rows where surprise is NaN or -- (indicating no surprise)\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != '--']\n",
    "# remove surprise values that are NaN or 0\n",
    "df_surprise = df_surprise[df_surprise['Surprise'].notna() & (df_surprise['Surprise'] != 0)]\n",
    "# Change surprise column to numeric, coercing errors\n",
    "df_surprise['Surprise'] = pd.to_numeric(df_surprise['Surprise'], errors='coerce')\n",
    "\n",
    "# Drop rows where Time is NaN\n",
    "df_surprise.dropna(subset=['Time'], inplace=True)\n",
    "\n",
    "# Convert Unnamed: 0 column to datetime\n",
    "df_surprise['Unnamed: 0'] = pd.to_datetime(df_surprise['Unnamed: 0'], errors='coerce')\n",
    "# Combine datetime column with Time column and set as index\n",
    "df_surprise['DateTime'] = pd.to_datetime(df_surprise['Unnamed: 0'].astype(str) + ' ' + df_surprise['Time'], errors='coerce')\n",
    "df_surprise.set_index('DateTime', inplace=True)\n",
    "# Drop the original Unnamed: 0 and Time columns\n",
    "df_surprise.drop(columns=['Unnamed: 0', 'Time'], inplace=True)\n",
    "# Sort the index\n",
    "df_surprise.sort_index(inplace=True)\n",
    "\n",
    "# remove surprise values that are NaN or 0\n",
    "df_surprise = df_surprise[df_surprise['Surprise'].notna() & (df_surprise['Surprise'] != 0)]\n",
    "\n",
    "# Wincorsizing to get results between 0.5% and 99.5% percentile for Surprise values\n",
    "\n",
    "lower_bound = df_surprise['Surprise'].quantile(0.005)\n",
    "upper_bound = df_surprise['Surprise'].quantile(0.995)\n",
    "\n",
    "df_surprise = df_surprise[(df_surprise['Surprise'] >= lower_bound) & (df_surprise['Surprise'] <= upper_bound)]\n",
    "\n",
    "\n",
    "# Combining the 2 dataframes on the index\n",
    "# outer join the two DataFrames on the index\n",
    "df_combined = pd.merge(df_es, df_surprise, left_index=True, right_index=True, how='outer', suffixes=('_es', '_surprise'))\n",
    "\n",
    "# remove all rows with NaN values in Open Close / Volume - Do not take out volume, can be 0 (not necessarily a mistake)\n",
    "df_combined.dropna(subset=['Open', 'Close'], inplace=True)\n",
    "\n",
    "# Adding a column called 'Volume_L1' which is the Volume shifted by 1 day - which is the volume of the previous minute matched up with the current minute\n",
    "df_combined['Volume_L1'] = df_combined['Volume'].shift(1)\n",
    "\n",
    "# Now we create the technical explanatory variables based on literature\n",
    "\n",
    "def create_technical_features(df):\n",
    "    \"\"\"\n",
    "    Creates all technical features for a dataframe containing price/volume data\n",
    "    and three return columns (Return, Return_half, Return_double)\n",
    "    \"\"\"\n",
    "    # 1. Original Features\n",
    "    df_combined['Volume'] = df_combined['Volume']\n",
    "    df_combined['Price'] = df_combined['Open']\n",
    "\n",
    "    # 2. Simple Moving Averages (now includes all required windows)\n",
    "    ma_windows = [5, 10, 15, 20, 50, 100, 200]  # Added missing windows for crossovers\n",
    "    for window in ma_windows:\n",
    "        df[f'SMA{window}'] = df['Close'].rolling(window).mean() \n",
    "\n",
    "    # 3. Moving Average Crossovers (now all SMAs exist)\n",
    "    for window in [5, 10, 15, 20, 50, 100, 200]:\n",
    "        # No more need for existence check since we created all SMAs\n",
    "        df[f'SMA{window}Cross'] = (df['Close'] > df[f'SMA{window}']).astype(int) # 1 for above SMA, 0 for below\n",
    "\n",
    "    # 4. Consecutive Price Trends\n",
    "    trend_periods = [10, 15, 50]\n",
    "    for period in trend_periods:\n",
    "        df[f'UpDown{period}'] = np.sign(df['Close'].pct_change(period)) # 1 for up, -1 for down, NaN for no change\n",
    "\n",
    "    # Ensure we keep the original return columns\n",
    "    return_cols = ['Return', 'Return_half', 'Return_double']\n",
    "    for col in return_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col]  # Maintain existing returns\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df_combined must contain: 'volume', 'close' columns plus the 3 return columns\n",
    "df_combined = create_technical_features(df_combined)\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "# --- 1. Set Start and End Dates, matches up with last date of df_combined ---\n",
    "start = df_combined.index.min() # Get the first date from df_combined\n",
    "end = df_combined.index.max()  # Get the last date from df_combined\n",
    "\n",
    "# print start and end\n",
    "print(f\"Start Date: {start}, End Date: {end}\")\n",
    "\n",
    "# --- 2. Get GDP Growth Data ---\n",
    "gdp_gr = pdr.DataReader('A191RL1Q225SBEA', 'fred', start, end)\n",
    "\n",
    "# Create gdp_gr_ml dataframe\n",
    "gdp_gr_ml = gdp_gr.reset_index()\n",
    "gdp_gr_ml.rename(columns={'A191RL1Q225SBEA': 'gdp_gr'}, inplace=True)\n",
    "gdp_gr_ml['DateTime'] = pd.to_datetime(gdp_gr_ml['DATE']) + pd.Timedelta(hours=23, minutes=59)\n",
    "gdp_gr_ml = gdp_gr_ml[['DateTime', 'gdp_gr']]\n",
    "\n",
    "# for both datasets set 'DateTime' as index and then drop columns\n",
    "gdp_gr_ml.set_index('DateTime', inplace=True)\n",
    "\n",
    "# make sure sorted on datetime\n",
    "gdp_gr_ml.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# Combine with Final Dataframe\n",
    "\n",
    "# Merge the last known GDP growth merge on index\n",
    "df_combined = pd.merge_asof(\n",
    "    df_combined,\n",
    "    gdp_gr_ml,\n",
    "    on='DateTime',\n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Rename the merged columns to last_gdp_gr and last_vix\n",
    "df_combined = df_combined.rename(columns={\n",
    "    'gdp_gr': 'last_gdp_gr'\n",
    "})\n",
    "\n",
    "# Set DateTime column as Index\n",
    "df_combined.set_index('DateTime', inplace=True)\n",
    "\n",
    "def calculate_daily_realized_volatility(df, freq='5min', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculates the daily realized volatility (non-annualized) from intraday data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with a datetime index and a close price column.\n",
    "        freq (str): The sampling frequency (e.g., '1min', '5min', '15min').\n",
    "        close_col (str): The name of the close price column.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series of daily (non-annualized) realized volatility.\n",
    "    \"\"\"\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a DatetimeIndex.\")\n",
    "        \n",
    "    # Resample the close price to the desired frequency\n",
    "    df_resampled = df[close_col].resample(freq).last()\n",
    "\n",
    "    # Calculate log returns\n",
    "    log_returns = np.log(df_resampled).diff().dropna()\n",
    "\n",
    "    # Group by day and calculate the sum of squared returns (realized variance)\n",
    "    realized_variance_daily = (log_returns**2).resample('D').sum()\n",
    "    \n",
    "    # Filter out days with no trading activity\n",
    "    realized_variance_daily = realized_variance_daily[realized_variance_daily > 0]\n",
    "    \n",
    "    # Calculate daily volatility (standard deviation), which is the square root of variance.\n",
    "    # The result is NOT annualized, making it suitable for ML models.\n",
    "    realized_volatility_daily = np.sqrt(realized_variance_daily)\n",
    "    \n",
    "    return realized_volatility_daily.dropna()\n",
    "\n",
    "\n",
    "def analyze_and_add_volatility(df_combined):\n",
    "    \"\"\"\n",
    "    Calculates daily realized volatility and adds it as a new column\n",
    "    to the provided DataFrame in-place.\n",
    "\n",
    "    Args:\n",
    "        df_combined (pd.DataFrame): Your DataFrame with minute-level data.\n",
    "                                    It must have a DatetimeIndex and a 'Close' column.\n",
    "                                    This function modifies the DataFrame in-place.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the new 'daily realized volatility' column.\n",
    "    \"\"\"\n",
    "    # --- 1. Calculate Volatility ---\n",
    "    # We use a 5-minute interval as a standard practice to balance capturing\n",
    "    # volatility information and reducing market microstructure noise.\n",
    "    # You can change this value if needed.\n",
    "    sampling_interval = '5min'\n",
    "    print(f\"Calculating daily realized volatility using a '{sampling_interval}' sampling interval...\")\n",
    "    \n",
    "    daily_volatility = calculate_daily_realized_volatility(\n",
    "        df_combined, \n",
    "        freq=sampling_interval, \n",
    "        close_col='Close' # Using 'Close' with a capital C\n",
    "    )\n",
    "\n",
    "    # --- 2. Add Volatility Column to DataFrame ---\n",
    "    # Map the calculated daily volatility back to the original minute-level DataFrame.\n",
    "    # Each row will have the realized volatility value for its corresponding day.\n",
    "    print(\"Adding 'daily realized volatility' column to the DataFrame...\")\n",
    "    # Normalize index to match the daily frequency of the volatility series\n",
    "    df_combined['daily realized volatility'] = df_combined.index.normalize().map(daily_volatility)\n",
    "    \n",
    "    print(\"\\n'daily realized volatility' column has been added successfully.\")\n",
    "        \n",
    "    return df_combined\n",
    "\n",
    "\n",
    "# --- HOW TO USE ---\n",
    "if __name__ == '__main__':\n",
    "    # 1. Make sure your DataFrame `df_combined` is loaded and available.\n",
    "    #    It must have a DatetimeIndex and columns named:\n",
    "    #    'open', 'high', 'low', 'Close', 'tick_count', 'volume'.\n",
    "    #\n",
    "    # Example of loading your data:\n",
    "    # df_combined = pd.read_csv('path/to/your/futures_data.csv', \n",
    "    #                           parse_dates=['timestamp_column'], \n",
    "    #                           index_col='timestamp_column')\n",
    "\n",
    "    # 2. Once `df_combined` is loaded, uncomment and run the following line:\n",
    "    # df_combined_with_vol = analyze_and_add_volatility(df_combined)\n",
    "    \n",
    "    print(\"Script is ready. Please load your 'df_combined' DataFrame and call 'analyze_and_add_volatility(df_combined)' to start.\")\n",
    "    \n",
    "    # For demonstration, if a variable named `df_combined` exists, we'll run the analysis.\n",
    "    if 'df_combined' in locals() or 'df_combined' in globals():\n",
    "        print(\"\\n'df_combined' detected. Running analysis...\")\n",
    "        df_combined_with_vol = analyze_and_add_volatility(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa5d709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Tick Count</th>\n",
       "      <th>Event</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Prior</th>\n",
       "      <th>Revised</th>\n",
       "      <th>...</th>\n",
       "      <th>SMA15Cross</th>\n",
       "      <th>SMA20Cross</th>\n",
       "      <th>SMA50Cross</th>\n",
       "      <th>SMA100Cross</th>\n",
       "      <th>SMA200Cross</th>\n",
       "      <th>UpDown10</th>\n",
       "      <th>UpDown15</th>\n",
       "      <th>UpDown50</th>\n",
       "      <th>last_gdp_gr</th>\n",
       "      <th>daily realized volatility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-09-10 00:01:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10 00:02:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10 00:03:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10 00:04:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10 00:05:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-19 15:56:00</th>\n",
       "      <td>5941.75</td>\n",
       "      <td>5942.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>5941.75</td>\n",
       "      <td>318.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-19 15:57:00</th>\n",
       "      <td>5941.75</td>\n",
       "      <td>5942.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>386.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-19 15:58:00</th>\n",
       "      <td>5941.50</td>\n",
       "      <td>5941.50</td>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>484.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-19 15:59:00</th>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>5940.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-19 16:00:00</th>\n",
       "      <td>5941.00</td>\n",
       "      <td>5942.75</td>\n",
       "      <td>5941.00</td>\n",
       "      <td>5941.25</td>\n",
       "      <td>8864.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.010728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9695665 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open     High      Low    Close  Volume  Tick Count  \\\n",
       "DateTime                                                                      \n",
       "1997-09-10 00:01:00     0.00     0.00     0.00     0.00     0.0         0.0   \n",
       "1997-09-10 00:02:00     0.00     0.00     0.00     0.00     0.0         0.0   \n",
       "1997-09-10 00:03:00     0.00     0.00     0.00     0.00     0.0         0.0   \n",
       "1997-09-10 00:04:00     0.00     0.00     0.00     0.00     0.0         0.0   \n",
       "1997-09-10 00:05:00     0.00     0.00     0.00     0.00     0.0         0.0   \n",
       "...                      ...      ...      ...      ...     ...         ...   \n",
       "2024-12-19 15:56:00  5941.75  5942.00  5941.25  5941.75   318.0        79.0   \n",
       "2024-12-19 15:57:00  5941.75  5942.00  5941.25  5941.50   386.0       100.0   \n",
       "2024-12-19 15:58:00  5941.50  5941.50  5940.75  5941.00   484.0       122.0   \n",
       "2024-12-19 15:59:00  5940.75  5941.25  5940.75  5941.00  6462.0       138.0   \n",
       "2024-12-19 16:00:00  5941.00  5942.75  5941.00  5941.25  8864.0       184.0   \n",
       "\n",
       "                    Event Ticker Prior Revised  ... SMA15Cross SMA20Cross  \\\n",
       "DateTime                                        ...                         \n",
       "1997-09-10 00:01:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "1997-09-10 00:02:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "1997-09-10 00:03:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "1997-09-10 00:04:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "1997-09-10 00:05:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "...                   ...    ...   ...     ...  ...        ...        ...   \n",
       "2024-12-19 15:56:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "2024-12-19 15:57:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "2024-12-19 15:58:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "2024-12-19 15:59:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "2024-12-19 16:00:00   NaN    NaN   NaN     NaN  ...          0          0   \n",
       "\n",
       "                    SMA50Cross SMA100Cross  SMA200Cross UpDown10  UpDown15  \\\n",
       "DateTime                                                                     \n",
       "1997-09-10 00:01:00          0           0            0      NaN       NaN   \n",
       "1997-09-10 00:02:00          0           0            0      NaN       NaN   \n",
       "1997-09-10 00:03:00          0           0            0      NaN       NaN   \n",
       "1997-09-10 00:04:00          0           0            0      NaN       NaN   \n",
       "1997-09-10 00:05:00          0           0            0      NaN       NaN   \n",
       "...                        ...         ...          ...      ...       ...   \n",
       "2024-12-19 15:56:00          1           0            0     -1.0      -1.0   \n",
       "2024-12-19 15:57:00          1           0            0     -1.0       1.0   \n",
       "2024-12-19 15:58:00          1           0            0     -1.0      -1.0   \n",
       "2024-12-19 15:59:00          1           0            0     -1.0      -1.0   \n",
       "2024-12-19 16:00:00          1           0            0     -1.0      -1.0   \n",
       "\n",
       "                    UpDown50  last_gdp_gr  daily realized volatility  \n",
       "DateTime                                                              \n",
       "1997-09-10 00:01:00      NaN          NaN                        inf  \n",
       "1997-09-10 00:02:00      NaN          NaN                        inf  \n",
       "1997-09-10 00:03:00      NaN          NaN                        inf  \n",
       "1997-09-10 00:04:00      NaN          NaN                        inf  \n",
       "1997-09-10 00:05:00      NaN          NaN                        inf  \n",
       "...                      ...          ...                        ...  \n",
       "2024-12-19 15:56:00      1.0          2.4                   0.010728  \n",
       "2024-12-19 15:57:00      1.0          2.4                   0.010728  \n",
       "2024-12-19 15:58:00      1.0          2.4                   0.010728  \n",
       "2024-12-19 15:59:00      1.0          2.4                   0.010728  \n",
       "2024-12-19 16:00:00      1.0          2.4                   0.010728  \n",
       "\n",
       "[9695665 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ec145",
   "metadata": {},
   "source": [
    "Creating Target column and Forward Return column for performance calculation\n",
    "\n",
    "TC is included in the target / forward return column calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006081f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Assumed DataFrame ---\n",
    "# The following code assumes you have a DataFrame named 'df_combined'\n",
    "# with 'Open' and 'Close' price columns and a DateTime index.\n",
    "\n",
    "# --- Configuration ---\n",
    "# This value represents the base holding period. If your data is in minutes,\n",
    "# this would be 20 minutes.\n",
    "holding_period = 20 \n",
    "\n",
    "# Define the transaction cost as a percentage.\n",
    "# This is the updated round trip transaction cost.\n",
    "transaction_cost_pct = 0.00005\n",
    "\n",
    "# Define the horizons to loop through.\n",
    "horizons = ['', '_half', '_double']\n",
    "\n",
    "# --- Main Loop to Generate All Columns ---\n",
    "for h in horizons:\n",
    "    # 1. Determine the calculation period and descriptive minute value\n",
    "    if h == '_half':\n",
    "        period = holding_period // 2\n",
    "        minutes = holding_period // 2\n",
    "    elif h == '_double':\n",
    "        period = holding_period * 2\n",
    "        minutes = holding_period * 2\n",
    "    else: # h == ''\n",
    "        period = holding_period\n",
    "        minutes = holding_period\n",
    "\n",
    "    # 2. Define the new, descriptive column names\n",
    "    return_col_name = f'Forward Return ({minutes} min)'\n",
    "    target_col_name = f'Target Signal ({minutes} min)'\n",
    "\n",
    "    # 3. Calculate the base forward return using the 'Close' price\n",
    "    # It shifts the future 'Close' price back to the current row to represent the\n",
    "    # potential return from a trade initiated at the current 'Close' price.\n",
    "    future_price = df_combined['Close'].shift(-period)\n",
    "    current_price = df_combined['Close']\n",
    "    \n",
    "    # Base return for a long position (buy low, sell high)\n",
    "    long_return = (future_price / current_price) - 1\n",
    "    \n",
    "    # Base return for a short position (sell high, buy low)\n",
    "    short_return = (current_price / future_price) - 1\n",
    "    \n",
    "    # 4. Create the Forward Return column, adjusted for transaction costs\n",
    "    # For a long trade to be profitable, the return must be greater than the cost.\n",
    "    net_long_return = long_return - transaction_cost_pct\n",
    "    \n",
    "    # For a short trade to be profitable, its return must also be greater than the cost.\n",
    "    net_short_return = short_return - transaction_cost_pct\n",
    "    \n",
    "    # The final 'Forward Return' will be the net long return. This column\n",
    "    # represents the realistic, achievable profit after costs for a long position.\n",
    "    df_combined[return_col_name] = net_long_return\n",
    "\n",
    "    # 5. Create the categorical \"Target Signal\" column with a cost-based \"dead zone\"\n",
    "    # The model's target (y) is now determined by whether a trade is\n",
    "    # profitable *after* accounting for transaction costs.\n",
    "    # 0 = Hold, 1 = Buy, 2 = Sell\n",
    "    \n",
    "    # A \"buy\" signal is only generated if the net long return is positive.\n",
    "    cond_buy = net_long_return > 0\n",
    "    \n",
    "    # A \"sell\" signal is only generated if the net short return is positive.\n",
    "    cond_sell = net_short_return > 0\n",
    "\n",
    "    df_combined[target_col_name] = 0  # Default to Hold\n",
    "    df_combined.loc[cond_buy, target_col_name] = 1\n",
    "    df_combined.loc[cond_sell, target_col_name] = 2\n",
    "\n",
    "# The 'df_combined' DataFrame is now updated with the new columns,\n",
    "# calculated using the 'Close' price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2bb00",
   "metadata": {},
   "source": [
    "Create feature column for each Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412ed680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Filtering for rows with surprise events...\n",
      "Found 23068 rows with surprise events to pivot.\n",
      "\n",
      "Step 2: Pivoting the data...\n",
      "Pivoting complete.\n",
      "\n",
      "--- Event DataFrame Sample ---\n",
      "Ticker               ACNFCOMF Index_Surprise  ADP CHNG Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "Ticker               AHE MOM% Index_Surprise  AHE YOY% Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "Ticker               AMSPPACE Index_Surprise  AWH TOTL Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "Ticker               CFNAI Index_Surprise  CGNOXAI% Index_Surprise  \\\n",
      "DateTime                                                             \n",
      "1997-12-23 07:30:00                   0.0                      0.0   \n",
      "1998-03-26 07:30:00                   0.0                      0.0   \n",
      "1998-04-30 07:30:00                   0.0                      0.0   \n",
      "1998-06-01 09:00:00                   0.0                      0.0   \n",
      "1998-06-02 09:00:00                   0.0                      0.0   \n",
      "\n",
      "Ticker               CGSHXAI% Index_Surprise  CHPMINDX Index_Surprise  ...  \\\n",
      "DateTime                                                               ...   \n",
      "1997-12-23 07:30:00                      0.0                      0.0  ...   \n",
      "1998-03-26 07:30:00                      0.0                      0.0  ...   \n",
      "1998-04-30 07:30:00                      0.0                      0.0  ...   \n",
      "1998-06-01 09:00:00                      0.0                      0.0  ...   \n",
      "1998-06-02 09:00:00                      0.0                      0.0  ...   \n",
      "\n",
      "Ticker               USMMMNCH Index_Surprise  USPHTMOM Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "Ticker               USPHTYOY Index_Surprise  USTBTOT Index_Surprise  \\\n",
      "DateTime                                                               \n",
      "1997-12-23 07:30:00                      0.0                     0.0   \n",
      "1998-03-26 07:30:00                      0.0                     0.0   \n",
      "1998-04-30 07:30:00                      0.0                     0.0   \n",
      "1998-06-01 09:00:00                      0.0                     0.0   \n",
      "1998-06-02 09:00:00                      0.0                     0.0   \n",
      "\n",
      "Ticker               USTGTTCB Index_Surprise  USUDMAER Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "Ticker               USURTOT Index_Surprise  USWHMANS Index_Surprise  \\\n",
      "DateTime                                                               \n",
      "1997-12-23 07:30:00                     0.0                      0.0   \n",
      "1998-03-26 07:30:00                     0.0                      0.0   \n",
      "1998-04-30 07:30:00                     0.0                      0.0   \n",
      "1998-06-01 09:00:00                     0.0                      0.0   \n",
      "1998-06-02 09:00:00                     0.0                      0.0   \n",
      "\n",
      "Ticker               USWHTOT Index_Surprise  VNCCCMOM Index_Surprise  \n",
      "DateTime                                                              \n",
      "1997-12-23 07:30:00                     0.0                      0.0  \n",
      "1998-03-26 07:30:00                     0.0                      0.0  \n",
      "1998-04-30 07:30:00                     0.0                      0.0  \n",
      "1998-06-01 09:00:00                     0.0                      0.0  \n",
      "1998-06-02 09:00:00                     0.0                      0.0  \n",
      "\n",
      "[5 rows x 161 columns]\n",
      "\n",
      "Shape of the new event DataFrame: (10038, 161)\n",
      "Number of rows matches the number of events, and number of columns matches unique tickers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This script assumes 'df_combined' is an existing DataFrame with a DateTimeIndex\n",
    "# and columns 'Ticker' and 'Surprise'.\n",
    "\n",
    "# --- 1. Filter for Event Rows Only ---\n",
    "# First, create a smaller dataframe that only contains rows where a surprise occurred.\n",
    "# This makes the pivot operation much more efficient.\n",
    "print(\"Step 1: Filtering for rows with surprise events...\")\n",
    "surprise_events_only = df_combined[df_combined['Surprise'].notna()].copy()\n",
    "print(f\"Found {len(surprise_events_only)} rows with surprise events to pivot.\")\n",
    "\n",
    "\n",
    "# --- 2. Pivot the Filtered Data ---\n",
    "# Now, we pivot this smaller dataframe.\n",
    "# - The index of the new dataframe will be the original DateTimeIndex.\n",
    "# - New columns will be created from the unique values in the 'Ticker' column.\n",
    "# - The values for these new columns will be taken from the 'Surprise' column.\n",
    "# - `fill_value=0` is critical: it ensures that for any given event row, all ticker\n",
    "#   columns that were NOT part of that event are filled with 0.\n",
    "print(\"\\nStep 2: Pivoting the data...\")\n",
    "event_df = surprise_events_only.pivot_table(\n",
    "    index=surprise_events_only.index,\n",
    "    columns='Ticker',\n",
    "    values='Surprise',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Optional: Add a suffix to the new column names for clarity\n",
    "event_df = event_df.add_suffix('_Surprise')\n",
    "print(\"Pivoting complete.\")\n",
    "\n",
    "\n",
    "# --- 3. Verify the Result ---\n",
    "print(\"\\n--- Event DataFrame Sample ---\")\n",
    "print(event_df.head())\n",
    "\n",
    "print(f\"\\nShape of the new event DataFrame: {event_df.shape}\")\n",
    "print(f\"Number of rows matches the number of events, and number of columns matches unique tickers.\")\n",
    "\n",
    "# The `event_df` is now ready. It only contains rows from event times,\n",
    "# and each ticker has its own column filled with either the surprise value or 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join complete.\n",
      "\n",
      "--- Final ML-Ready DataFrame ---\n",
      "                     ACNFCOMF Index_Surprise  ADP CHNG Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "                     AHE MOM% Index_Surprise  AHE YOY% Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "                     AMSPPACE Index_Surprise  AWH TOTL Index_Surprise  \\\n",
      "DateTime                                                                \n",
      "1997-12-23 07:30:00                      0.0                      0.0   \n",
      "1998-03-26 07:30:00                      0.0                      0.0   \n",
      "1998-04-30 07:30:00                      0.0                      0.0   \n",
      "1998-06-01 09:00:00                      0.0                      0.0   \n",
      "1998-06-02 09:00:00                      0.0                      0.0   \n",
      "\n",
      "                     CFNAI Index_Surprise  CGNOXAI% Index_Surprise  \\\n",
      "DateTime                                                             \n",
      "1997-12-23 07:30:00                   0.0                      0.0   \n",
      "1998-03-26 07:30:00                   0.0                      0.0   \n",
      "1998-04-30 07:30:00                   0.0                      0.0   \n",
      "1998-06-01 09:00:00                   0.0                      0.0   \n",
      "1998-06-02 09:00:00                   0.0                      0.0   \n",
      "\n",
      "                     CGSHXAI% Index_Surprise  CHPMINDX Index_Surprise  ...  \\\n",
      "DateTime                                                               ...   \n",
      "1997-12-23 07:30:00                      0.0                      0.0  ...   \n",
      "1998-03-26 07:30:00                      0.0                      0.0  ...   \n",
      "1998-04-30 07:30:00                      0.0                      0.0  ...   \n",
      "1998-06-01 09:00:00                      0.0                      0.0  ...   \n",
      "1998-06-02 09:00:00                      0.0                      0.0  ...   \n",
      "\n",
      "                           R  Volume_L1  Surprise  Std Dev  \\\n",
      "DateTime                                                     \n",
      "1997-12-23 07:30:00  96.2963        0.0     -2.98     0.07   \n",
      "1998-03-26 07:30:00  96.2963        0.0     -2.52     0.08   \n",
      "1998-04-30 07:30:00  96.2963        0.0      2.22     0.36   \n",
      "1998-06-01 09:00:00  94.8148        0.0     -1.88     0.77   \n",
      "1998-06-02 09:00:00  88.1481        0.0      3.69    13.02   \n",
      "\n",
      "                     Forward Return (20 min)  Target Signal (20 min)  \\\n",
      "DateTime                                                               \n",
      "1997-12-23 07:30:00                 0.000000                       0   \n",
      "1998-03-26 07:30:00                -0.000457                       2   \n",
      "1998-04-30 07:30:00                 0.012548                       1   \n",
      "1998-06-01 09:00:00                 0.003439                       1   \n",
      "1998-06-02 09:00:00                 0.000454                       1   \n",
      "\n",
      "                     Forward Return (10 min)  Target Signal (10 min)  \\\n",
      "DateTime                                                               \n",
      "1997-12-23 07:30:00                 0.000261                       1   \n",
      "1998-03-26 07:30:00                 0.001571                       1   \n",
      "1998-04-30 07:30:00                 0.009807                       1   \n",
      "1998-06-01 09:00:00                 0.003206                       1   \n",
      "1998-06-02 09:00:00                 0.000454                       1   \n",
      "\n",
      "                     Forward Return (40 min)  Target Signal (40 min)  \n",
      "DateTime                                                              \n",
      "1997-12-23 07:30:00                -0.000261                       2  \n",
      "1998-03-26 07:30:00                -0.000229                       2  \n",
      "1998-04-30 07:30:00                 0.013235                       1  \n",
      "1998-06-01 09:00:00                 0.003206                       1  \n",
      "1998-06-02 09:00:00                -0.001594                       2  \n",
      "\n",
      "[5 rows x 190 columns]\n",
      "\n",
      "Shape of the final DataFrame: (23068, 190)\n",
      "The number of rows (23068) should match the number of surprise events.\n",
      "\n",
      "Final columns available for the model:\n",
      "['ACNFCOMF Index_Surprise', 'ADP CHNG Index_Surprise', 'AHE MOM% Index_Surprise', 'AHE YOY% Index_Surprise', 'AMSPPACE Index_Surprise', 'AWH TOTL Index_Surprise', 'CFNAI Index_Surprise', 'CGNOXAI% Index_Surprise', 'CGSHXAI% Index_Surprise', 'CHPMINDX Index_Surprise', 'CICRTOT Index_Surprise', 'CNSTTMOM Index_Surprise', 'COMFCOMF Index_Surprise', 'CONCCONF Index_Surprise', 'CONSCURR Index_Surprise', 'CONSEXP Index_Surprise', 'CONSP5MD Index_Surprise', 'CONSPXMD Index_Surprise', 'CONSSENT Index_Surprise', 'COSTNFR% Index_Surprise', 'CPI CHNG Index_Surprise', 'CPI XYOY Index_Surprise', 'CPI YOY Index_Surprise', 'CPTICHNG Index_Surprise', 'CPUPAXFE Index_Surprise', 'CPUPXCHG Index_Surprise', 'CPURNSA Index_Surprise', 'DFEDGBA Index_Surprise', 'DGNOCHNG Index_Surprise', 'DGNOXTCH Index_Surprise', 'DOTDLTMD Index_Surprise', 'DOTDY0MD Index_Surprise', 'DOTDY1MD Index_Surprise', 'DOTDY2MD Index_Surprise', 'DSERGBCC Index_Surprise', 'ECI SA% Index_Surprise', 'ECONUSIB Index_Surprise', 'EHSLSL Index_Surprise', 'EMPRGBCI Index_Surprise', 'ETSLMOM Index_Surprise', 'ETSLTOTL Index_Surprise', 'EXP1CMOM Index_Surprise', 'EXP1CYOY Index_Surprise', 'FDDSSD Index_Surprise', 'FDEBTY Index_Surprise', 'FDIDFDMO Index_Surprise', 'FDIDSGMO Index_Surprise', 'FDIDSGUM Index_Surprise', 'FDIUFDYO Index_Surprise', 'FDIUSGUY Index_Surprise', 'FDIUSGYO Index_Surprise', 'FDTR Index_Surprise', 'FDTRFTRL Index_Surprise', 'FRNTTNET Index_Surprise', 'FRNTTOTL Index_Surprise', 'GDP CQOQ Index_Surprise', 'GDP DCHG Index_Surprise', 'GDP PIQQ Index_Surprise', 'GDPCPCEC Index_Surprise', 'GDPCTOT% Index_Surprise', 'HPI PURQ Index_Surprise', 'HPI QOQ% Index_Surprise', 'HPIMMOM% Index_Surprise', 'IMP1CHNG Index_Surprise', 'IMP1XPM% Index_Surprise', 'IMP1YOY% Index_Surprise', 'INJCJC Index_Surprise', 'INJCSP Index_Surprise', 'IP CHNG Index_Surprise', 'IPMGCHNG Index_Surprise', 'IRRBIOER Index_Surprise', 'JOLTTOTL Index_Surprise', 'KCLSSACI Index_Surprise', 'LEI CHNG Index_Surprise', 'LHWANWPA Index_Surprise', 'LMCILMCC Index_Surprise', 'MAPMINDX Index_Surprise', 'MBAVCHNG Index_Surprise', 'MBRXYOY Index_Surprise', 'MBRXYOYW Index_Surprise', 'MPMIUSCA Index_Surprise', 'MPMIUSMA Index_Surprise', 'MPMIUSSA Index_Surprise', 'MTIBCHNG Index_Surprise', 'MWINCHNG Index_Surprise', 'MWSLCHNG Index_Surprise', 'NAPMEMPL Index_Surprise', 'NAPMNEMP Index_Surprise', 'NAPMNEWO Index_Surprise', 'NAPMNMAN Index_Surprise', 'NAPMNMI Index_Surprise', 'NAPMNNO Index_Surprise', 'NAPMNPRC Index_Surprise', 'NAPMPMI Index_Surprise', 'NAPMPRIC Index_Surprise', 'NFP CPYC Index_Surprise', 'NFP PCH Index_Surprise', 'NFP TCH Index_Surprise', 'NHCHATCH Index_Surprise', 'NHCHSTCH Index_Surprise', 'NHSLCHNG Index_Surprise', 'NHSLTOT Index_Surprise', 'NHSPATOT Index_Surprise', 'NHSPSTOT Index_Surprise', 'NYBLCNBA Index_Surprise', 'NYCNM1IR Index_Surprise', 'NYPMCURR Index_Surprise', 'OUTFGAF Index_Surprise', 'PCE CHNC Index_Surprise', 'PCE CMOM Index_Surprise', 'PCE CRCH Index_Surprise', 'PCE CYOY Index_Surprise', 'PCE DEFM Index_Surprise', 'PCE DEFY Index_Surprise', 'PHUCTOT Index_Surprise', 'PITLCHNG Index_Surprise', 'PNMARADI Index_Surprise', 'PPI CHNG Index_Surprise', 'PPI XYOY Index_Surprise', 'PPI YOY Index_Surprise', 'PRODNFR% Index_Surprise', 'PRUSTOT Index_Surprise', 'PXFECHNG Index_Surprise', 'RCHSINDX Index_Surprise', 'REALRAWE Index_Surprise', 'REALYRAE Index_Surprise', 'RSRSTMOM Index_Surprise', 'RSTAMOM Index_Surprise', 'RSTAXAG% Index_Surprise', 'RSTAXAGM Index_Surprise', 'RSTAXMOM Index_Surprise', 'RTSDCHNG Index_Surprise', 'RTSDXCHG Index_Surprise', 'SAARDTOT Index_Surprise', 'SAARTOTL Index_Surprise', 'SBOITOTL Index_Surprise', 'SPCS20 Index_Surprise', 'SPCS20SM Index_Surprise', 'SPCS20Y% Index_Surprise', 'SPCSUSA Index_Surprise', 'SPCSUSAY Index_Surprise', 'SPCSUSQS Index_Surprise', 'TMNOCHNG Index_Surprise', 'TMNOXTM% Index_Surprise', 'TREFAMSP Index_Surprise', 'TREFPACE Index_Surprise', 'USCABAL Index_Surprise', 'USEMNCHG Index_Surprise', 'USHBMIDX Index_Surprise', 'USHETOT% Index_Surprise', 'USHEYOY Index_Surprise', 'USMMMNCH Index_Surprise', 'USPHTMOM Index_Surprise', 'USPHTYOY Index_Surprise', 'USTBTOT Index_Surprise', 'USTGTTCB Index_Surprise', 'USUDMAER Index_Surprise', 'USURTOT Index_Surprise', 'USWHMANS Index_Surprise', 'USWHTOT Index_Surprise', 'VNCCCMOM Index_Surprise', 'SMA5', 'SMA10', 'SMA15', 'SMA20', 'SMA50', 'SMA100', 'SMA200', 'SMA5Cross', 'SMA10Cross', 'SMA15Cross', 'SMA20Cross', 'SMA50Cross', 'SMA100Cross', 'SMA200Cross', 'UpDown10', 'UpDown15', 'UpDown50', 'last_gdp_gr', 'daily realized volatility', 'R', 'Volume_L1', 'Surprise', 'Std Dev', 'Forward Return (20 min)', 'Target Signal (20 min)', 'Forward Return (10 min)', 'Target Signal (10 min)', 'Forward Return (40 min)', 'Target Signal (40 min)']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Selecting feature and target columns from df_combined\n",
    "# all columns with 'SMA' at start or 'UpDown' and 'Last_GDP_GR' and 'Last_VIX' at the end and\n",
    "feature_columns = [col for col in df_combined.columns if col.startswith('SMA') or col.startswith('UpDown') or col in ['last_gdp_gr', 'daily realized volatility']]\n",
    "columns_to_join = feature_columns\n",
    "\n",
    "# also include the 'R' and 'Volume_L1' and 'Surprise' columns\n",
    "columns_to_join += ['R', 'Volume_L1', 'Surprise', 'Std Dev']\n",
    "\n",
    "# also include the return columns\n",
    "list = [\n",
    "    'Forward Return (20 min)',\n",
    "    'Target Signal (20 min)',\n",
    "    'Forward Return (10 min)',\n",
    "    'Target Signal (10 min)',\n",
    "    'Forward Return (40 min)',\n",
    "    'Target Signal (40 min)'\n",
    "]\n",
    "\n",
    "columns_to_join += list\n",
    "\n",
    "features_and_target_from_combined = df_combined[columns_to_join]\n",
    "\n",
    "ml_df = event_df.join(features_and_target_from_combined, how='inner')\n",
    "\n",
    "# Clean up any potential NaNs that might arise from the join, just in case.\n",
    "ml_df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"Join complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Verify the Final DataFrame ---\n",
    "print(\"\\n--- Final ML-Ready DataFrame ---\")\n",
    "print(ml_df.head())\n",
    "\n",
    "print(f\"\\nShape of the final DataFrame: {ml_df.shape}\")\n",
    "print(f\"The number of rows ({ml_df.shape[0]}) should match the number of surprise events.\")\n",
    "print(\"\\nFinal columns available for the model:\")\n",
    "print(ml_df.columns.tolist())\n",
    "\n",
    "# The `ml_df` DataFrame is now complete and ready to be split into X (all columns except Target)\n",
    "# and y (the Target column) to be fed into your XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dadb030e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateTime</th>\n",
       "      <th>ACNFCOMF Index_Surprise</th>\n",
       "      <th>ADP CHNG Index_Surprise</th>\n",
       "      <th>AHE MOM% Index_Surprise</th>\n",
       "      <th>AHE YOY% Index_Surprise</th>\n",
       "      <th>AMSPPACE Index_Surprise</th>\n",
       "      <th>AWH TOTL Index_Surprise</th>\n",
       "      <th>CFNAI Index_Surprise</th>\n",
       "      <th>CGNOXAI% Index_Surprise</th>\n",
       "      <th>CGSHXAI% Index_Surprise</th>\n",
       "      <th>...</th>\n",
       "      <th>R</th>\n",
       "      <th>Volume_L1</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Forward Return (20 min)</th>\n",
       "      <th>Target Signal (20 min)</th>\n",
       "      <th>Forward Return (10 min)</th>\n",
       "      <th>Target Signal (10 min)</th>\n",
       "      <th>Forward Return (40 min)</th>\n",
       "      <th>Target Signal (40 min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-12-23 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.296300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.98</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998-03-26 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.296300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998-04-30 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.296300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998-06-01 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.814800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-06-02 09:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.148100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.69</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001594</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23063</th>\n",
       "      <td>2024-12-18 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.185200</td>\n",
       "      <td>131.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>22.21</td>\n",
       "      <td>-0.001505</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.002970</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23064</th>\n",
       "      <td>2024-12-18 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.851900</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1.78</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.003010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23065</th>\n",
       "      <td>2024-12-18 07:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.111100</td>\n",
       "      <td>131.0</td>\n",
       "      <td>-3.84</td>\n",
       "      <td>6.22</td>\n",
       "      <td>-0.001546</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000529</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.002847</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23066</th>\n",
       "      <td>2024-12-18 13:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.222220</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.006478</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.010023</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23067</th>\n",
       "      <td>2024-12-18 13:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>801.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.008475</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.007171</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.009493</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23068 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 DateTime  ACNFCOMF Index_Surprise  ADP CHNG Index_Surprise  \\\n",
       "0     1997-12-23 07:30:00                      0.0                      0.0   \n",
       "1     1998-03-26 07:30:00                      0.0                      0.0   \n",
       "2     1998-04-30 07:30:00                      0.0                      0.0   \n",
       "3     1998-06-01 09:00:00                      0.0                      0.0   \n",
       "4     1998-06-02 09:00:00                      0.0                      0.0   \n",
       "...                   ...                      ...                      ...   \n",
       "23063 2024-12-18 07:30:00                      0.0                      0.0   \n",
       "23064 2024-12-18 07:30:00                      0.0                      0.0   \n",
       "23065 2024-12-18 07:30:00                      0.0                      0.0   \n",
       "23066 2024-12-18 13:00:00                      0.0                      0.0   \n",
       "23067 2024-12-18 13:00:00                      0.0                      0.0   \n",
       "\n",
       "       AHE MOM% Index_Surprise  AHE YOY% Index_Surprise  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "...                        ...                      ...   \n",
       "23063                      0.0                      0.0   \n",
       "23064                      0.0                      0.0   \n",
       "23065                      0.0                      0.0   \n",
       "23066                      0.0                      0.0   \n",
       "23067                      0.0                      0.0   \n",
       "\n",
       "       AMSPPACE Index_Surprise  AWH TOTL Index_Surprise  CFNAI Index_Surprise  \\\n",
       "0                          0.0                      0.0                   0.0   \n",
       "1                          0.0                      0.0                   0.0   \n",
       "2                          0.0                      0.0                   0.0   \n",
       "3                          0.0                      0.0                   0.0   \n",
       "4                          0.0                      0.0                   0.0   \n",
       "...                        ...                      ...                   ...   \n",
       "23063                      0.0                      0.0                   0.0   \n",
       "23064                      0.0                      0.0                   0.0   \n",
       "23065                      0.0                      0.0                   0.0   \n",
       "23066                      0.0                      0.0                   0.0   \n",
       "23067                      0.0                      0.0                   0.0   \n",
       "\n",
       "       CGNOXAI% Index_Surprise  CGSHXAI% Index_Surprise  ...          R  \\\n",
       "0                          0.0                      0.0  ...  96.296300   \n",
       "1                          0.0                      0.0  ...  96.296300   \n",
       "2                          0.0                      0.0  ...  96.296300   \n",
       "3                          0.0                      0.0  ...  94.814800   \n",
       "4                          0.0                      0.0  ...  88.148100   \n",
       "...                        ...                      ...  ...        ...   \n",
       "23063                      0.0                      0.0  ...  61.185200   \n",
       "23064                      0.0                      0.0  ...  31.851900   \n",
       "23065                      0.0                      0.0  ...  71.111100   \n",
       "23066                      0.0                      0.0  ...   2.222220   \n",
       "23067                      0.0                      0.0  ...   0.740741   \n",
       "\n",
       "       Volume_L1  Surprise  Std Dev  Forward Return (20 min)  \\\n",
       "0            0.0     -2.98     0.07                 0.000000   \n",
       "1            0.0     -2.52     0.08                -0.000457   \n",
       "2            0.0      2.22     0.36                 0.012548   \n",
       "3            0.0     -1.88     0.77                 0.003439   \n",
       "4            0.0      3.69    13.02                 0.000454   \n",
       "...          ...       ...      ...                      ...   \n",
       "23063      131.0      3.38    22.21                -0.001505   \n",
       "23064      131.0      2.87     1.78                -0.001668   \n",
       "23065      131.0     -3.84     6.22                -0.001546   \n",
       "23066     1239.0      2.03     0.12                -0.008963   \n",
       "23067      801.0      2.35     0.11                -0.008475   \n",
       "\n",
       "       Target Signal (20 min)  Forward Return (10 min)  \\\n",
       "0                           0                 0.000261   \n",
       "1                           2                 0.001571   \n",
       "2                           1                 0.009807   \n",
       "3                           1                 0.003206   \n",
       "4                           1                 0.000454   \n",
       "...                       ...                      ...   \n",
       "23063                       2                -0.000325   \n",
       "23064                       2                -0.000488   \n",
       "23065                       2                -0.000529   \n",
       "23066                       2                -0.006478   \n",
       "23067                       2                -0.007171   \n",
       "\n",
       "       Target Signal (10 min)  Forward Return (40 min)  Target Signal (40 min)  \n",
       "0                           1                -0.000261                       2  \n",
       "1                           1                -0.000229                       2  \n",
       "2                           1                 0.013235                       1  \n",
       "3                           1                 0.003206                       1  \n",
       "4                           1                -0.001594                       2  \n",
       "...                       ...                      ...                     ...  \n",
       "23063                       2                -0.002970                       2  \n",
       "23064                       2                -0.003010                       2  \n",
       "23065                       2                -0.002847                       2  \n",
       "23066                       2                -0.010023                       2  \n",
       "23067                       2                -0.009493                       2  \n",
       "\n",
       "[23068 rows x 191 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename ml_df to df_ml\n",
    "df_ml = ml_df\n",
    "\n",
    "# adjust df_ml so that index, DateTime, become a single column\n",
    "df_ml.reset_index(inplace=True)\n",
    "\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2455a",
   "metadata": {},
   "source": [
    "<h1>XGBoost model</h1>\n",
    "- Without transaction cost\n",
    "- Rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dad4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: ['ACNFCOMF Index_Surprise', 'ADP CHNG Index_Surprise', 'AHE MOM% Index_Surprise', 'AHE YOY% Index_Surprise', 'AMSPPACE Index_Surprise', 'AWH TOTL Index_Surprise', 'CFNAI Index_Surprise', 'CGNOXAI% Index_Surprise', 'CGSHXAI% Index_Surprise', 'CHPMINDX Index_Surprise', 'CICRTOT Index_Surprise', 'CNSTTMOM Index_Surprise', 'COMFCOMF Index_Surprise', 'CONCCONF Index_Surprise', 'CONSCURR Index_Surprise', 'CONSEXP Index_Surprise', 'CONSP5MD Index_Surprise', 'CONSPXMD Index_Surprise', 'CONSSENT Index_Surprise', 'COSTNFR% Index_Surprise', 'CPI CHNG Index_Surprise', 'CPI XYOY Index_Surprise', 'CPI YOY Index_Surprise', 'CPTICHNG Index_Surprise', 'CPUPAXFE Index_Surprise', 'CPUPXCHG Index_Surprise', 'CPURNSA Index_Surprise', 'DFEDGBA Index_Surprise', 'DGNOCHNG Index_Surprise', 'DGNOXTCH Index_Surprise', 'DOTDLTMD Index_Surprise', 'DOTDY0MD Index_Surprise', 'DOTDY1MD Index_Surprise', 'DOTDY2MD Index_Surprise', 'DSERGBCC Index_Surprise', 'ECI SA% Index_Surprise', 'ECONUSIB Index_Surprise', 'EHSLSL Index_Surprise', 'EMPRGBCI Index_Surprise', 'ETSLMOM Index_Surprise', 'ETSLTOTL Index_Surprise', 'EXP1CMOM Index_Surprise', 'EXP1CYOY Index_Surprise', 'FDDSSD Index_Surprise', 'FDEBTY Index_Surprise', 'FDIDFDMO Index_Surprise', 'FDIDSGMO Index_Surprise', 'FDIDSGUM Index_Surprise', 'FDIUFDYO Index_Surprise', 'FDIUSGUY Index_Surprise', 'FDIUSGYO Index_Surprise', 'FDTR Index_Surprise', 'FDTRFTRL Index_Surprise', 'FRNTTNET Index_Surprise', 'FRNTTOTL Index_Surprise', 'GDP CQOQ Index_Surprise', 'GDP DCHG Index_Surprise', 'GDP PIQQ Index_Surprise', 'GDPCPCEC Index_Surprise', 'GDPCTOT% Index_Surprise', 'HPI PURQ Index_Surprise', 'HPI QOQ% Index_Surprise', 'HPIMMOM% Index_Surprise', 'IMP1CHNG Index_Surprise', 'IMP1XPM% Index_Surprise', 'IMP1YOY% Index_Surprise', 'INJCJC Index_Surprise', 'INJCSP Index_Surprise', 'IP CHNG Index_Surprise', 'IPMGCHNG Index_Surprise', 'IRRBIOER Index_Surprise', 'JOLTTOTL Index_Surprise', 'KCLSSACI Index_Surprise', 'LEI CHNG Index_Surprise', 'LHWANWPA Index_Surprise', 'LMCILMCC Index_Surprise', 'MAPMINDX Index_Surprise', 'MBAVCHNG Index_Surprise', 'MBRXYOY Index_Surprise', 'MBRXYOYW Index_Surprise', 'MPMIUSCA Index_Surprise', 'MPMIUSMA Index_Surprise', 'MPMIUSSA Index_Surprise', 'MTIBCHNG Index_Surprise', 'MWINCHNG Index_Surprise', 'MWSLCHNG Index_Surprise', 'NAPMEMPL Index_Surprise', 'NAPMNEMP Index_Surprise', 'NAPMNEWO Index_Surprise', 'NAPMNMAN Index_Surprise', 'NAPMNMI Index_Surprise', 'NAPMNNO Index_Surprise', 'NAPMNPRC Index_Surprise', 'NAPMPMI Index_Surprise', 'NAPMPRIC Index_Surprise', 'NFP CPYC Index_Surprise', 'NFP PCH Index_Surprise', 'NFP TCH Index_Surprise', 'NHCHATCH Index_Surprise', 'NHCHSTCH Index_Surprise', 'NHSLCHNG Index_Surprise', 'NHSLTOT Index_Surprise', 'NHSPATOT Index_Surprise', 'NHSPSTOT Index_Surprise', 'NYBLCNBA Index_Surprise', 'NYCNM1IR Index_Surprise', 'NYPMCURR Index_Surprise', 'OUTFGAF Index_Surprise', 'PCE CHNC Index_Surprise', 'PCE CMOM Index_Surprise', 'PCE CRCH Index_Surprise', 'PCE CYOY Index_Surprise', 'PCE DEFM Index_Surprise', 'PCE DEFY Index_Surprise', 'PHUCTOT Index_Surprise', 'PITLCHNG Index_Surprise', 'PNMARADI Index_Surprise', 'PPI CHNG Index_Surprise', 'PPI XYOY Index_Surprise', 'PPI YOY Index_Surprise', 'PRODNFR% Index_Surprise', 'PRUSTOT Index_Surprise', 'PXFECHNG Index_Surprise', 'RCHSINDX Index_Surprise', 'REALRAWE Index_Surprise', 'REALYRAE Index_Surprise', 'RSRSTMOM Index_Surprise', 'RSTAMOM Index_Surprise', 'RSTAXAG% Index_Surprise', 'RSTAXAGM Index_Surprise', 'RSTAXMOM Index_Surprise', 'RTSDCHNG Index_Surprise', 'RTSDXCHG Index_Surprise', 'SAARDTOT Index_Surprise', 'SAARTOTL Index_Surprise', 'SBOITOTL Index_Surprise', 'SPCS20 Index_Surprise', 'SPCS20SM Index_Surprise', 'SPCS20Y% Index_Surprise', 'SPCSUSA Index_Surprise', 'SPCSUSAY Index_Surprise', 'SPCSUSQS Index_Surprise', 'TMNOCHNG Index_Surprise', 'TMNOXTM% Index_Surprise', 'TREFAMSP Index_Surprise', 'TREFPACE Index_Surprise', 'USCABAL Index_Surprise', 'USEMNCHG Index_Surprise', 'USHBMIDX Index_Surprise', 'USHETOT% Index_Surprise', 'USHEYOY Index_Surprise', 'USMMMNCH Index_Surprise', 'USPHTMOM Index_Surprise', 'USPHTYOY Index_Surprise', 'USTBTOT Index_Surprise', 'USTGTTCB Index_Surprise', 'USUDMAER Index_Surprise', 'USURTOT Index_Surprise', 'USWHMANS Index_Surprise', 'USWHTOT Index_Surprise', 'VNCCCMOM Index_Surprise', 'SMA5', 'SMA10', 'SMA15', 'SMA20', 'SMA50', 'SMA100', 'SMA200', 'SMA5Cross', 'SMA10Cross', 'SMA15Cross', 'SMA20Cross', 'SMA50Cross', 'SMA100Cross', 'SMA200Cross', 'UpDown10', 'UpDown15', 'UpDown50', 'last_gdp_gr', 'daily realized volatility', 'R', 'Volume_L1', 'Surprise', 'Std Dev']\n",
      "Number of Features: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:186: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_returns = val_trade_df.groupby('DateTime').apply(\n",
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_26196\\370654438.py:212: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_returns = test_trade_df.groupby('DateTime').apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cumulative Compounded Return: 812.19%\n",
      "\n",
      "Overall Test Accuracy : 52.04%\n",
      "Overall Precision     : 38.97%\n",
      "Overall Recall        : 36.47%\n",
      "Overall F1 Score      : 36.30%\n",
      "Overall ROC AUC       : 58.80%\n",
      "\n",
      "Average Fold Test Return : 0.03185%\n",
      "\n",
      "Standard Deviation of All Test Returns (Aggregated Daily): 0.24900%\n",
      "\n",
      "Minimum Return (Aggregated Daily): -0.96503%\n",
      "\n",
      "Optimal Horizon Counts:\n",
      "(40 min)    11\n",
      "(20 min)     4\n",
      "(10 min)     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Set Position Counts:\n",
      "Neutral (0): 80\n",
      "Long (1): 8921\n",
      "Short (2): 8247\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assume df_ml is pre-loaded as a pandas DataFrame.\n",
    "\n",
    "# feature_cols = all columns except 'DateTime' and the last 6 columns and also print out feature_cols and number of features\n",
    "feature_cols = df_ml.columns[:-6].tolist()\n",
    "feature_cols.remove('DateTime')\n",
    "print(\"Feature Columns:\", feature_cols)\n",
    "print(\"Number of Features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "horizons = [' (20 min)', ' (10 min)', ' (40 min)']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial Training Window ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "# MODIFICATION: Define both a start and end date for the window\n",
    "train_start_date = min_date\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    longs = active_trades[active_trades['pred'] == 1]\n",
    "    shorts = active_trades[active_trades['pred'] == 2]\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx, row in longs.iterrows():\n",
    "            raw_return = row[f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx, row in shorts.iterrows():\n",
    "            raw_return = -row[f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns)\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    # MODIFICATION: The training mask now uses both a start and end date for a rolling window\n",
    "    train_mask = (df_ml['DateTime'] >= train_start_date) & (df_ml['DateTime'] <= train_end_date)\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty or train_data.empty:\n",
    "        print(f\"Skipping period starting {val_start_date.date()} (no data)\")\n",
    "        # MODIFICATION: Move the entire window forward\n",
    "        train_start_date += pd.DateOffset(years=1)\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Target Signal{horizon}']\n",
    "        y_val = val_data[f'Target Signal{horizon}']\n",
    "\n",
    "        # Train model\n",
    "        model = xgb.XGBClassifier(\n",
    "            num_class=3,\n",
    "            n_estimators=60,\n",
    "            max_depth=7,\n",
    "            random_state=42,\n",
    "            seed=42\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        # Calculate training stop loss\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(train_preds, model.predict_proba(X_train))],\n",
    "            f'Forward Return{horizon}': train_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        individual_train_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                raw = row[f'Forward Return{horizon}']\n",
    "            elif row['pred'] == 2:\n",
    "                raw = -row[f'Forward Return{horizon}']\n",
    "            else:\n",
    "                continue\n",
    "            individual_train_returns.append(raw)\n",
    "\n",
    "        stop_loss_train = np.percentile(individual_train_returns, 5) if individual_train_returns else None\n",
    "\n",
    "        # Calculate validation stop loss (for test set)\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(val_preds, val_probas)],\n",
    "            f'Forward Return{horizon}': val_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        individual_val_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                raw = row[f'Forward Return{horizon}']\n",
    "            elif row['pred'] == 2:\n",
    "                raw = -row[f'Forward Return{horizon}']\n",
    "            else:\n",
    "                continue\n",
    "            individual_val_returns.append(raw)\n",
    "\n",
    "        stop_loss_val = np.percentile(individual_val_returns, 5) if individual_val_returns else None\n",
    "\n",
    "        # Validate using TRAINING stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Target Signal{best_horizon}'].values\n",
    "\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [p[pred] for pred, p in zip(test_preds, test_probas)],\n",
    "            f'Forward Return{best_horizon}': test_data[f'Forward Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    # MODIFICATION: Increment both start and end dates to slide the window forward\n",
    "    train_start_date += pd.DateOffset(years=1)\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (XGBoost with Rolling Window)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdc906",
   "metadata": {},
   "source": [
    "<h1>Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493929f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assume df_ml is pre-loaded as a pandas DataFrame.\n",
    "\n",
    "# feature_cols = all columns except 'DateTime' and the last 6 columns and also print out feature_cols and number of features\n",
    "feature_cols = df_ml.columns[:-6].tolist()\n",
    "feature_cols.remove('DateTime')\n",
    "print(\"Feature Columns:\", feature_cols)\n",
    "print(\"Number of Features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "horizons = [' (20 min)', ' (10 min)', ' (40 min)']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial Training Window ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "# MODIFICATION: Define both a start and end date for the window\n",
    "train_start_date = min_date\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Create copies with reset index\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs with reset index\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts with reset index\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns)\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    # MODIFICATION: The training mask now uses both a start and end date for a rolling window\n",
    "    train_mask = (df_ml['DateTime'] >= train_start_date) & (df_ml['DateTime'] <= train_end_date)\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty or train_data.empty:\n",
    "        print(f\"Skipping period starting {val_start_date.date()} (no data)\")\n",
    "        # MODIFICATION: Move the entire window forward\n",
    "        train_start_date += pd.DateOffset(years=1)\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Target Signal{horizon}']\n",
    "        y_val = val_data[f'Target Signal{horizon}']\n",
    "\n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            criterion='gini',\n",
    "            min_samples_leaf=4,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Get class probabilities\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_probas = model.predict_proba(X_train)\n",
    "        class_map = {cls: idx for idx, cls in enumerate(model.classes_)}\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(train_preds, train_probas) if pred in class_map],\n",
    "            f'Forward Return{horizon}': train_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(val_preds, val_probas) if pred in class_map],\n",
    "            f'Forward Return{horizon}': val_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        # Validation stop loss calculation (for test set)\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Target Signal{best_horizon}'].values\n",
    "\n",
    "        class_map = {cls: idx for idx, cls in enumerate(best_model.classes_)}\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(test_preds, test_probas) if pred in class_map],\n",
    "            f'Forward Return{best_horizon}': test_data[f'Forward Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    # MODIFICATION: Increment both start and end dates to slide the window forward\n",
    "    train_start_date += pd.DateOffset(years=1)\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (Random Forest with Rolling Window)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    if len(np.unique(all_test_truths)) > 1:\n",
    "        print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Overall ROC AUC       : Not enough data for calculation.\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f6f12",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assume df_ml is pre-loaded as a pandas DataFrame.\n",
    "\n",
    "# feature_cols = all columns except 'DateTime' and the last 6 columns and also print out feature_cols and number of features\n",
    "feature_cols = df_ml.columns[:-6].tolist()\n",
    "feature_cols.remove('DateTime')\n",
    "print(\"Feature Columns:\", feature_cols)\n",
    "print(\"Number of Features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "horizons = [' (20 min)', ' (10 min)', ' (40 min)']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial Training Window ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "# MODIFICATION: Define both a start and end date for the window\n",
    "train_start_date = min_date\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    # Create copies with reset index\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs with capping\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts with capping\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns)\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    # MODIFICATION: The training mask now uses both a start and end date for a rolling window\n",
    "    train_mask = (df_ml['DateTime'] >= train_start_date) & (df_ml['DateTime'] <= train_end_date)\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty or train_data.empty:\n",
    "        print(f\"Skipping period starting {val_start_date.date()} (no data)\")\n",
    "        # MODIFICATION: Move the entire window forward\n",
    "        train_start_date += pd.DateOffset(years=1)\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss = None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "        y_train = train_data[f'Target Signal{horizon}']\n",
    "        y_val = val_data[f'Target Signal{horizon}']\n",
    "\n",
    "        # Train Logistic Regression\n",
    "        model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='saga',\n",
    "            max_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_probas = model.predict_proba(X_train)\n",
    "        class_map = {cls: idx for idx, cls in enumerate(model.classes_)}\n",
    "\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(train_preds, train_probas) if pred in class_map],\n",
    "            f'Forward Return{horizon}': train_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions and stop loss\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(val_preds, val_probas) if pred in class_map],\n",
    "            f'Forward Return{horizon}': val_data[f'Forward Return{horizon}']\n",
    "        })\n",
    "\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        X_test = scaler.transform(test_data[feature_cols])\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        y_test = test_data[f'Target Signal{best_horizon}'].values\n",
    "\n",
    "        class_map = {cls: idx for idx, cls in enumerate(best_model.classes_)}\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[class_map.get(pred)] for pred, proba in zip(test_preds, test_probas) if pred in class_map],\n",
    "            f'Forward Return{best_horizon}': test_data[f'Forward Return{best_horizon}']\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    # MODIFICATION: Increment both start and end dates to slide the window forward\n",
    "    train_start_date += pd.DateOffset(years=1)\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (Logistic Regression with Rolling Window)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    if len(np.unique(all_test_truths)) > 1:\n",
    "        print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Overall ROC AUC       : Not enough data for calculation.\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d08607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Add this after your existing print statements\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                    columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                           target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713dc877",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TensorFlow INFO and WARNING messages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Assume df_ml is pre-loaded as a pandas DataFrame in your environment.\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "all_cols = df_ml.columns.tolist()\n",
    "exclude_cols = ['DateTime', 'Surprise'] + [col for col in all_cols if 'Target' in col or 'Forward' in col]\n",
    "feature_cols = [col for col in all_cols if col not in exclude_cols]\n",
    "print(\"Feature Columns:\", feature_cols)\n",
    "print(\"Number of Features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "horizons = [' (20 min)', ' (10 min)', ' (40 min)']\n",
    "N_CLASSES = 3 # Assuming classes are 0, 1, 2\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial Training Window ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "# MODIFICATION: Define both a start and end date for the window\n",
    "train_start_date = min_date\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"Creates a simple LSTM model.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(50, input_shape=input_shape, return_sequences=False, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns)\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    # MODIFICATION: The training mask now uses both a start and end date for a rolling window\n",
    "    train_mask = (df_ml['DateTime'] >= train_start_date) & (df_ml['DateTime'] <= train_end_date)\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty or train_data.empty:\n",
    "        print(f\"Skipping period starting {val_start_date.date()} (no data)\")\n",
    "        # MODIFICATION: Move the entire window forward\n",
    "        train_start_date += pd.DateOffset(years=1)\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss, best_scaler = None, None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\nEvaluating Horizon: {horizon} for training period {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val_scaled = scaler.transform(val_data[feature_cols])\n",
    "\n",
    "        # Reshape data for LSTM: [samples, timesteps, features]\n",
    "        X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "        X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "\n",
    "        y_train = train_data[f'Target Signal{horizon}']\n",
    "        y_val = val_data[f'Target Signal{horizon}']\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        y_train_cat = to_categorical(y_train, num_classes=N_CLASSES)\n",
    "        y_val_cat = to_categorical(y_val, num_classes=N_CLASSES)\n",
    "\n",
    "        # Create and Train LSTM model\n",
    "        model = create_lstm_model(input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), num_classes=N_CLASSES)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train_reshaped, y_train_cat,\n",
    "                  validation_data=(X_val_reshaped, y_val_cat),\n",
    "                  epochs=50,\n",
    "                  batch_size=32,\n",
    "                  callbacks=[early_stopping],\n",
    "                  verbose=0)\n",
    "\n",
    "        # Get class probabilities\n",
    "        train_probas = model.predict(X_train_reshaped)\n",
    "        train_preds = np.argmax(train_probas, axis=1)\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[pred] for pred, proba in zip(train_preds, train_probas)],\n",
    "            f'Forward Return{horizon}': train_data[f'Forward Return{horizon}'].values\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions\n",
    "        val_probas = model.predict(X_val_reshaped)\n",
    "        val_preds = np.argmax(val_probas, axis=1)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[pred] for pred, proba in zip(val_preds, val_probas)],\n",
    "            f'Forward Return{horizon}': val_data[f'Forward Return{horizon}'].values\n",
    "        })\n",
    "\n",
    "        # Validation stop loss calculation (for test set)\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "            best_scaler = scaler\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        print(f\"\\n--- Testing best model for period ending {test_end_date.date()} ---\")\n",
    "        print(f\"Best Horizon Found: {best_horizon}\")\n",
    "        \n",
    "        X_test_scaled = best_scaler.transform(test_data[feature_cols])\n",
    "        X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "        \n",
    "        test_probas = best_model.predict(X_test_reshaped)\n",
    "        test_preds = np.argmax(test_probas, axis=1)\n",
    "        y_test = test_data[f'Target Signal{best_horizon}'].values\n",
    "\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[pred] for pred, proba in zip(test_preds, test_probas)],\n",
    "            f'Forward Return{best_horizon}': test_data[f'Forward Return{best_horizon}'].values\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    # MODIFICATION: Increment both start and end dates to slide the window forward\n",
    "    train_start_date += pd.DateOffset(years=1)\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_dates, sorted_returns = zip(*sorted(zip(all_dates, cumulative_returns)))\n",
    "    plt.plot(sorted_dates, [r * 100 for r in sorted_returns])\n",
    "    plt.title('Cumulative Returns (LSTM with Rolling Window)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    full_probas = np.vstack(all_test_probas)\n",
    "    y_test_cat = to_categorical(all_test_truths, num_classes=N_CLASSES)\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    if len(np.unique(all_test_truths)) > 1:\n",
    "        print(f\"Overall ROC AUC       : {roc_auc_score(y_test_cat, full_probas, multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Overall ROC AUC       : Not enough data for calculation.\")\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                        index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                        columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "print(cm_df)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_test_truths, all_test_preds,\n",
    "                                target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9f072",
   "metadata": {},
   "source": [
    "<h1>SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Assume df_ml is pre-loaded as a pandas DataFrame in your environment.\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "all_cols = df_ml.columns.tolist()\n",
    "exclude_cols = ['DateTime', 'Surprise'] + [col for col in all_cols if 'Target' in col or 'Forward' in col]\n",
    "feature_cols = [col for col in all_cols if col not in exclude_cols]\n",
    "print(\"Feature Columns:\", feature_cols)\n",
    "print(\"Number of Features:\", len(feature_cols))\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "horizons = [' (20 min)', ' (10 min)', ' (40 min)']\n",
    "\n",
    "# --- Data Prep ---\n",
    "df_ml = df_ml.dropna(subset=['Surprise']).sort_values('DateTime')\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "\n",
    "# --- Calculate Initial Training Window ---\n",
    "total_duration = max_date - min_date\n",
    "initial_train_duration = total_duration * 0.3\n",
    "# MODIFICATION: Define both a start and end date for the window\n",
    "train_start_date = min_date\n",
    "train_end_date = min_date + initial_train_duration\n",
    "\n",
    "\n",
    "# --- Tracking containers ---\n",
    "results = []\n",
    "all_test_returns = []\n",
    "all_dates = []\n",
    "all_test_preds = []\n",
    "all_test_truths = []\n",
    "all_test_probas = []\n",
    "fold_mean_returns = []\n",
    "best_horizons_list = []\n",
    "\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon, stop_loss=None):\n",
    "    \"\"\"Calculate returns with position sizing and stop loss\"\"\"\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "\n",
    "    longs = active_trades[active_trades['pred'] == 1].copy().reset_index(drop=True)\n",
    "    shorts = active_trades[active_trades['pred'] == 2].copy().reset_index(drop=True)\n",
    "\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "\n",
    "    long_returns = 0.0\n",
    "    short_returns = 0.0\n",
    "\n",
    "    # Process longs\n",
    "    if not longs.empty:\n",
    "        long_weights = longs['proba'] / total_confidence\n",
    "        for idx in longs.index:\n",
    "            raw_return = longs.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            long_returns += capped_raw * long_weights.loc[idx]\n",
    "\n",
    "    # Process shorts\n",
    "    if not shorts.empty:\n",
    "        short_weights = shorts['proba'] / total_confidence\n",
    "        for idx in shorts.index:\n",
    "            raw_return = -shorts.loc[idx, f'Forward Return{horizon}']\n",
    "            capped_raw = max(stop_loss, raw_return) if stop_loss is not None else raw_return\n",
    "            short_returns += capped_raw * short_weights.loc[idx]\n",
    "\n",
    "    net_return = (long_returns + short_returns)\n",
    "    return net_return\n",
    "\n",
    "# --- Main Loop ---\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    # MODIFICATION: The training mask now uses both a start and end date for a rolling window\n",
    "    train_mask = (df_ml['DateTime'] >= train_start_date) & (df_ml['DateTime'] <= train_end_date)\n",
    "    val_mask = (df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)\n",
    "    test_mask = (df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)\n",
    "\n",
    "    train_data = df_ml[train_mask]\n",
    "    val_data = df_ml[val_mask]\n",
    "    test_data = df_ml[test_mask]\n",
    "\n",
    "    if val_data.empty or test_data.empty or train_data.empty:\n",
    "        print(f\"Skipping period starting {val_start_date.date()} (no data)\")\n",
    "        # Move the entire window forward\n",
    "        train_start_date += pd.DateOffset(years=1)\n",
    "        train_end_date += pd.DateOffset(years=1)\n",
    "        continue\n",
    "\n",
    "    # --- Horizon Optimization ---\n",
    "    best_horizon, best_model, best_stop_loss, best_scaler = None, None, None, None\n",
    "    best_val_return = -np.inf\n",
    "\n",
    "    for horizon in horizons:\n",
    "        print(f\"\\nEvaluating Horizon: {horizon} for training period {train_start_date.date()} to {train_end_date.date()}\")\n",
    "        # Feature scaling - SVMs are sensitive to feature scales\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(train_data[feature_cols])\n",
    "        X_val = scaler.transform(val_data[feature_cols])\n",
    "\n",
    "        y_train = train_data[f'Target Signal{horizon}']\n",
    "        y_val = val_data[f'Target Signal{horizon}']\n",
    "\n",
    "        # Create and Train SVM model\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Get class probabilities\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_probas = model.predict_proba(X_train)\n",
    "        class_map = {cls: idx for idx, cls in enumerate(model.classes_)}\n",
    "\n",
    "        # Training stop loss calculation\n",
    "        train_trade_df = pd.DataFrame({\n",
    "            'DateTime': train_data['DateTime'],\n",
    "            'pred': train_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(train_preds, train_probas)],\n",
    "            f'Forward Return{horizon}': train_data[f'Forward Return{horizon}'].values\n",
    "        })\n",
    "\n",
    "        train_raw_returns = []\n",
    "        for _, row in train_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                train_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                train_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_train = np.percentile(train_raw_returns, 5) if train_raw_returns else None\n",
    "\n",
    "        # Validation predictions\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_probas = model.predict_proba(X_val)\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'],\n",
    "            'pred': val_preds,\n",
    "            'proba': [proba[class_map.get(pred, 0)] for pred, proba in zip(val_preds, val_probas) if pred in class_map],\n",
    "            f'Forward Return{horizon}': val_data[f'Forward Return{horizon}'].values\n",
    "        })\n",
    "\n",
    "        # Validation stop loss calculation (for test set)\n",
    "        val_raw_returns = []\n",
    "        for _, row in val_trade_df.iterrows():\n",
    "            if row['pred'] == 1:\n",
    "                val_raw_returns.append(row[f'Forward Return{horizon}'])\n",
    "            elif row['pred'] == 2:\n",
    "                val_raw_returns.append(-row[f'Forward Return{horizon}'])\n",
    "        stop_loss_val = np.percentile(val_raw_returns, 5) if val_raw_returns else None\n",
    "\n",
    "        # Validate using training stop loss\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, horizon, stop_loss_train)\n",
    "        ).values\n",
    "\n",
    "        total_val_return = (1 + val_returns).prod() - 1\n",
    "\n",
    "        if total_val_return > best_val_return:\n",
    "            best_val_return = total_val_return\n",
    "            best_horizon = horizon\n",
    "            best_model = model\n",
    "            best_stop_loss = stop_loss_val\n",
    "            best_scaler = scaler\n",
    "\n",
    "    # --- Test Best Model ---\n",
    "    if best_model and best_stop_loss is not None:\n",
    "        print(f\"\\n--- Testing best model for period ending {test_end_date.date()} ---\")\n",
    "        print(f\"Best Horizon Found: {best_horizon}\")\n",
    "        X_test = best_scaler.transform(test_data[feature_cols])\n",
    "        \n",
    "        test_probas = best_model.predict_proba(X_test)\n",
    "        test_preds = best_model.predict(X_test)\n",
    "        y_test = test_data[f'Target Signal{best_horizon}'].values\n",
    "        \n",
    "        class_map = {cls: idx for idx, cls in enumerate(best_model.classes_)}\n",
    "        test_trade_df = pd.DataFrame({\n",
    "            'DateTime': test_data['DateTime'],\n",
    "            'pred': test_preds,\n",
    "            'proba': [proba[class_map[pred]] for pred, proba in zip(test_preds, test_probas)],\n",
    "            f'Forward Return{best_horizon}': test_data[f'Forward Return{best_horizon}'].values\n",
    "        })\n",
    "\n",
    "        grouped_returns = test_trade_df.groupby('DateTime').apply(\n",
    "            lambda x: calculate_grouped_returns(x, best_horizon, best_stop_loss)\n",
    "        ).values\n",
    "\n",
    "        all_test_returns.extend(grouped_returns)\n",
    "        all_dates.extend(test_trade_df['DateTime'].unique().tolist())\n",
    "        all_test_preds.extend(test_preds)\n",
    "        all_test_truths.extend(y_test)\n",
    "        all_test_probas.append(test_probas)\n",
    "        fold_mean_returns.append(np.mean(grouped_returns))\n",
    "        best_horizons_list.append(best_horizon or 'standard')\n",
    "\n",
    "    # MODIFICATION: Increment both start and end dates to slide the window forward\n",
    "    train_start_date += pd.DateOffset(years=1)\n",
    "    train_end_date += pd.DateOffset(years=1)\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    std_dev_individual = np.std(all_test_returns) * 100\n",
    "    min_return = np.nanmin(all_test_returns) * 100 if all_test_returns else 0.0\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_dates, sorted_returns = zip(*sorted(zip(all_dates, cumulative_returns)))\n",
    "    plt.plot(sorted_dates, [r * 100 for r in sorted_returns])\n",
    "    plt.title('Cumulative Returns (SVM with Rolling Window)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    full_probas = np.vstack(all_test_probas)\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision     : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall        : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score      : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    if len(np.unique(all_test_truths)) > 1:\n",
    "         print(f\"Overall ROC AUC       : {roc_auc_score(all_test_truths, full_probas, multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Overall ROC AUC       : Not enough data for calculation (requires at least 2 classes).\")\n",
    "\n",
    "    print(f\"\\nAverage Fold Test Return : {np.mean(fold_mean_returns)*100:.5f}%\")\n",
    "    print(f\"\\nStandard Deviation of All Test Returns (Aggregated Daily): {std_dev_individual:.5f}%\")\n",
    "    print(f\"\\nMinimum Return (Aggregated Daily): {min_return:.5f}%\")\n",
    "    print(\"\\nOptimal Horizon Counts:\")\n",
    "    print(pd.Series(best_horizons_list).value_counts())\n",
    "    print(\"\\nTest Set Position Counts:\")\n",
    "    counts = pd.Series(all_test_preds).value_counts().sort_index()\n",
    "    print(f\"Neutral (0): {counts.get(0, 0)}\")\n",
    "    print(f\"Long (1): {counts.get(1, 0)}\")\n",
    "    print(f\"Short (2): {counts.get(2, 0)}\")\n",
    "\n",
    "    # ADDED: Confusion Matrix and Classification Report\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_test_truths, all_test_preds)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                         index=['Actual Neutral (0)', 'Actual Long (1)', 'Actual Short (2)'],\n",
    "                         columns=['Pred Neutral (0)', 'Pred Long (1)', 'Pred Short (2)'])\n",
    "    print(cm_df)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_test_truths, all_test_preds,\n",
    "                                  target_names=['Neutral (0)', 'Long (1)', 'Short (2)']))\n",
    "\n",
    "else:\n",
    "    print(\"No valid test periods found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
